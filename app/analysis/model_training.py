# app/analysis/model_training.py
# ml ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ - RandomForest í•™ìŠµ, ëª¨ë¸ í‰ê°€ ë° artifacts ìƒì„±

import os
import datetime
from math import sqrt

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ë¨¸ì‹ ëŸ¬ë‹ ê´€ë ¨
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    mean_squared_error, confusion_matrix, accuracy_score, precision_score,
    recall_score, f1_score, roc_curve, roc_auc_score, classification_report,
    average_precision_score, balanced_accuracy_score, ConfusionMatrixDisplay,
    precision_recall_curve, auc
)
import joblib
import json

# ì´ê³³ì— ëª¨ë¸ í•™ìŠµ ë° ë¶„ì„ í•¨ìˆ˜ë“¤ì„ ì‘ì„±í•˜ì„¸ìš”
from typing import Dict, Any, Tuple


def prepare_data(
    df: pd.DataFrame,
    feature_cols: list,
    target_col: str,
    test_size: float = 0.2,
    random_state: int = 42
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
    """
    ë°ì´í„° ì¤€ë¹„: ì…ë ¥(X), íƒ€ê¹ƒ(y) ë¶„ë¦¬ ë° í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 

    Args:
        df: ë°ì´í„°í”„ë ˆì„
        feature_cols: íŠ¹ì„± ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        target_col: íƒ€ê¹ƒ ì»¬ëŸ¼ëª…
        test_size: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.2)
        random_state: ëœë¤ ì‹œë“œ

    Returns:
        X_train, X_test, y_train, y_test
    """
    # ì…ë ¥(X), íƒ€ê¹ƒ(y) ë¶„ë¦¬
    X = df[feature_cols]
    y = df[target_col]

    # í•™ìŠµìš©/í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )

    print(f"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ")
    print(f"í›ˆë ¨ ë°ì´í„°: {X_train.shape}, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")
    print(f"í›ˆë ¨ ë¼ë²¨ ë¶„í¬:\n{y_train.value_counts()}")
    print(f"í…ŒìŠ¤íŠ¸ ë¼ë²¨ ë¶„í¬:\n{y_test.value_counts()}")

    return X_train, X_test, y_train, y_test


def scale_data(
    X_train: pd.DataFrame,
    X_test: pd.DataFrame
) -> Tuple[np.ndarray, np.ndarray, StandardScaler]:
    """
    ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (í‘œì¤€í™”)

    Args:
        X_train: í›ˆë ¨ ë°ì´í„°
        X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°

    Returns:
        X_train_scaled, X_test_scaled, scaler
    """
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    print(f"\nâœ… ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
    print(f"í›ˆë ¨ ë°ì´í„° shape: {X_train_scaled.shape}")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° shape: {X_test_scaled.shape}")

    return X_train_scaled, X_test_scaled, scaler


def train_random_forest(
    X_train: np.ndarray,
    y_train: pd.Series,
    n_estimators: int = 200,
    max_depth: int = None,
    min_samples_split: int = 2,
    min_samples_leaf: int = 1,
    random_state: int = 42
) -> RandomForestClassifier:
    """
    RandomForest ëª¨ë¸ í•™ìŠµ

    Args:
        X_train: í›ˆë ¨ ë°ì´í„°
        y_train: í›ˆë ¨ ë¼ë²¨
        n_estimators: íŠ¸ë¦¬ ê°œìˆ˜
        max_depth: ìµœëŒ€ ê¹Šì´
        min_samples_split: ë¶„í• ì„ ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        min_samples_leaf: ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        random_state: ëœë¤ ì‹œë“œ

    Returns:
        í•™ìŠµëœ RandomForest ëª¨ë¸
    """
    print(f"\nğŸŒ² RandomForest ëª¨ë¸ í•™ìŠµ ì¤‘...")

    rf_model = RandomForestClassifier(
        n_estimators=n_estimators,
        random_state=random_state,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        n_jobs=-1
    )

    rf_model.fit(X_train, y_train)

    print(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
    print(f"íŠ¸ë¦¬ ê°œìˆ˜: {n_estimators}")
    print(f"ìµœëŒ€ ê¹Šì´: {max_depth}")

    return rf_model


def evaluate_model(
    model: RandomForestClassifier,
    X_test: np.ndarray,
    y_test: pd.Series
) -> Dict[str, Any]:
    """
    ëª¨ë¸ í‰ê°€ ë° ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°

    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
        y_test: í…ŒìŠ¤íŠ¸ ë¼ë²¨

    Returns:
        ì„±ëŠ¥ í‰ê°€ ê²°ê³¼
    """
    # ì˜ˆì¸¡
    y_pred = model.predict(X_test)

    # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred)

    print(f"\nğŸ“Š [RandomForest ì„±ëŠ¥ ìš”ì•½]")
    print(f"Accuracy : {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall   : {recall:.4f}")
    print(f"F1-score : {f1:.4f}")
    print(f"AUC      : {auc:.4f}")
    print(f"\nDetailed report:\n{classification_report(y_test, y_pred, digits=4)}")

    return {
        "accuracy": round(accuracy, 4),
        "precision": round(precision, 4),
        "recall": round(recall, 4),
        "f1_score": round(f1, 4),
        "auc": round(auc, 4),
        "classification_report": classification_report(y_test, y_pred, output_dict=True),
        "predictions": y_pred.tolist()
    }


def train_and_evaluate_rf(
    df: pd.DataFrame,
    feature_cols: list,
    target_col: str,
    test_size: float = 0.2,
    n_estimators: int = 200,
    random_state: int = 42
) -> Dict[str, Any]:
    """
    RandomForest ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì „ì²´ íŒŒì´í”„ë¼ì¸

    Args:
        df: ë°ì´í„°í”„ë ˆì„
        feature_cols: íŠ¹ì„± ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        target_col: íƒ€ê¹ƒ ì»¬ëŸ¼ëª…
        test_size: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
        n_estimators: íŠ¸ë¦¬ ê°œìˆ˜
        random_state: ëœë¤ ì‹œë“œ

    Returns:
        í•™ìŠµ ë° í‰ê°€ ê²°ê³¼
    """
    print("\n" + "="*60)
    print("ğŸš€ RandomForest ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì‹œì‘")
    print("="*60)

    # 1. ë°ì´í„° ì¤€ë¹„
    X_train, X_test, y_train, y_test = prepare_data(
        df, feature_cols, target_col, test_size, random_state
    )

    # 2. ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    X_train_scaled, X_test_scaled, scaler = scale_data(X_train, X_test)

    # 3. ëª¨ë¸ í•™ìŠµ
    model = train_random_forest(
        X_train_scaled, y_train, n_estimators=n_estimators, random_state=random_state
    )

    # 4. ëª¨ë¸ í‰ê°€
    evaluation = evaluate_model(model, X_test_scaled, y_test)

    print("\n" + "="*60)
    print("âœ… RandomForest ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì™„ë£Œ")
    print("="*60)

    return {
        "model": model,
        "scaler": scaler,
        "evaluation": evaluation,
        "data_split": {
            "train_size": len(X_train),
            "test_size": len(X_test),
            "feature_cols": feature_cols,
            "target_col": target_col
        }
    }


# =========================================
#  ëª¨ë¸ ê²°ê³¼ë¬¼ ì €ì¥
# =========================================

def save_model_artifacts(
    model,
    model_name: str,
    X_test: np.ndarray,
    y_test: pd.Series,
    feature_names: list,
    output_dir: str = "artifacts"
) -> Dict[str, str]:
    """
    ëª¨ë¸ì˜ ëª¨ë“  ê²°ê³¼ë¬¼(ì§€í‘œ, ì´ë¯¸ì§€, ëª¨ë¸ íŒŒì¼)ì„ ì €ì¥

    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        model_name: ëª¨ë¸ ì´ë¦„
        X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
        y_test: í…ŒìŠ¤íŠ¸ ë¼ë²¨
        feature_names: íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬

    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë“¤
    """
    print("\n" + "="*60)
    print(f"ğŸ’¾ {model_name} ê²°ê³¼ë¬¼ ì €ì¥ ì‹œì‘")
    print("="*60)

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)

    model_tag = model_name.lower().replace("classifier", "").replace(" ", "")

    # ===== 1) ì˜ˆì¸¡ ë° ì ìˆ˜ ê³„ì‚° =====
    y_true = y_test.values if isinstance(y_test, pd.Series) else np.asarray(y_test)
    y_pred = model.predict(X_test)

    # ì–‘ì„± í´ë˜ìŠ¤(1)ì— ëŒ€í•œ í™•ë¥  ë˜ëŠ” ì ìˆ˜
    if hasattr(model, "predict_proba"):
        y_score = model.predict_proba(X_test)[:, 1]
    else:
        y_score = model.decision_function(X_test)

    # í˜¼ë™í–‰ë ¬
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()

    # ì£¼ìš” ì§€í‘œ ê³„ì‚°
    metrics = {
        "model": model_name,
        "accuracy": accuracy_score(y_true, y_pred),
        "precision": precision_score(y_true, y_pred),
        "recall": recall_score(y_true, y_pred),
        "f1_score": f1_score(y_true, y_pred),
        "roc_auc": roc_auc_score(y_true, y_score),
        "avg_precision_AP": average_precision_score(y_true, y_score),
        "balanced_accuracy": balanced_accuracy_score(y_true, y_pred),
        "specificity": tn / (tn + fp + 1e-12),
        "n_test_samples": len(y_true),
        "n_positive": int(y_true.sum()),
        "n_negative": int((y_true == 0).sum()),
    }

    saved_files = {}

    # ===== 2) metrics ì €ì¥ =====
    metrics_path = os.path.join(output_dir, f"metrics_{model_tag}.csv")
    pd.DataFrame(list(metrics.items()), columns=["metric", "value"]).to_csv(
        metrics_path, index=False
    )
    saved_files["metrics_csv"] = metrics_path
    print(f"[ì €ì¥ ì™„ë£Œ] í‰ê°€ ì§€í‘œ â†’ {metrics_path}")

    # ===== 2-1) classification_report JSON ì €ì¥ =====
    report_dict = classification_report(y_true, y_pred, digits=4, output_dict=True)
    report_json_path = os.path.join(output_dir, f"classification_report_{model_tag}.json")
    with open(report_json_path, "w", encoding="utf-8") as f:
        json.dump(report_dict, f, indent=4, ensure_ascii=False)
    saved_files["classification_report_json"] = report_json_path
    print(f"[ì €ì¥ ì™„ë£Œ] ë¶„ë¥˜ ë¦¬í¬íŠ¸(JSON) â†’ {report_json_path}")

    # ===== 3) í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ =====
    pred_df = pd.DataFrame(
        {"y_true": y_true, "y_pred": y_pred, "y_score": y_score}
    )
    pred_path = os.path.join(output_dir, f"test_predictions_{model_tag}.csv")
    pred_df.to_csv(pred_path, index=False)
    saved_files["predictions"] = pred_path
    print(f"[ì €ì¥ ì™„ë£Œ] ì˜ˆì¸¡ ê²°ê³¼ â†’ {pred_path}")

    # ===== 4) í˜¼ë™í–‰ë ¬ ì´ë¯¸ì§€ & CSV =====
    disp = ConfusionMatrixDisplay.from_predictions(y_true, y_pred)
    plt.title(f"Confusion Matrix ({model_name})")
    cm_img_path = os.path.join(output_dir, f"confusion_matrix_{model_tag}.png")
    plt.savefig(cm_img_path, bbox_inches="tight")
    plt.close()
    saved_files["confusion_matrix_img"] = cm_img_path
    print(f"[ì €ì¥ ì™„ë£Œ] í˜¼ë™í–‰ë ¬ ì´ë¯¸ì§€ â†’ {cm_img_path}")

    cm_df = pd.DataFrame(
        cm, index=["true_0", "true_1"], columns=["pred_0", "pred_1"]
    )
    cm_csv_path = os.path.join(output_dir, f"confusion_matrix_{model_tag}.csv")
    cm_df.to_csv(cm_csv_path)
    saved_files["confusion_matrix_csv"] = cm_csv_path
    print(f"[ì €ì¥ ì™„ë£Œ] í˜¼ë™í–‰ë ¬ ê°’ â†’ {cm_csv_path}")

    # ===== 5) ROC Curve ì´ë¯¸ì§€ =====
    fpr, tpr, _ = roc_curve(y_true, y_score)
    roc_auc_val = auc(fpr, tpr)
    plt.figure(figsize=(5, 4))
    plt.plot(fpr, tpr, label=f"AUC={roc_auc_val:.3f}", color="darkorange")
    plt.plot([0, 1], [0, 1], "k--")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"ROC Curve ({model_name})")
    plt.legend()
    plt.grid(True)
    roc_path = os.path.join(output_dir, f"roc_curve_{model_tag}.png")
    plt.savefig(roc_path, bbox_inches="tight")
    plt.close()
    saved_files["roc_curve"] = roc_path
    print(f"[ì €ì¥ ì™„ë£Œ] ROC Curve ì´ë¯¸ì§€ â†’ {roc_path}")

    # ===== 6) Precisionâ€“Recall Curve ì´ë¯¸ì§€ =====
    prec, rec, _ = precision_recall_curve(y_true, y_score)
    ap = average_precision_score(y_true, y_score)
    plt.figure(figsize=(5, 4))
    plt.plot(rec, prec, color="blue", label=f"AP={ap:.3f}")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title(f"Precision-Recall Curve ({model_name})")
    plt.legend()
    plt.grid(True)
    pr_path = os.path.join(output_dir, f"pr_curve_{model_tag}.png")
    plt.savefig(pr_path, bbox_inches="tight")
    plt.close()
    saved_files["pr_curve"] = pr_path
    print(f"[ì €ì¥ ì™„ë£Œ] PR Curve ì´ë¯¸ì§€ â†’ {pr_path}")

    # ===== 7) Feature Importance (íŠ¸ë¦¬ ê³„ì—´ë§Œ) =====
    if hasattr(model, "feature_importances_"):
        fi = pd.Series(model.feature_importances_, index=feature_names).sort_values(
            ascending=False
        )
        fi_path = os.path.join(output_dir, f"feature_importance_{model_tag}.csv")
        fi.to_csv(fi_path, header=["importance"])
        saved_files["feature_importance"] = fi_path
        print(f"[ì €ì¥ ì™„ë£Œ] Feature Importance â†’ {fi_path}")

    # ===== 8) ëª¨ë¸ íŒŒì¼(joblib) =====
    model_path = os.path.join(output_dir, f"model_{model_tag}.joblib")
    joblib.dump(model, model_path)
    saved_files["model_file"] = model_path
    print(f"[ì €ì¥ ì™„ë£Œ] ëª¨ë¸ íŒŒì¼ â†’ {model_path}")

    print("="*60)
    print(f"âœ… {model_name} ê²°ê³¼ë¬¼ ì €ì¥ ì™„ë£Œ")
    print(f"ì €ì¥ ìœ„ì¹˜: {os.path.abspath(output_dir)}")
    print("="*60 + "\n")

    return saved_files
