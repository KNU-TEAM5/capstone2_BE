# app/analysis/preprocessing.py

import os
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Tuple
from scipy import stats


def read_csv_safely(path: str) -> pd.DataFrame:
    """
    ì—¬ëŸ¬ ì¸ì½”ë”©ì„ ì‹œë„í•˜ì—¬ CSV íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ì½ê¸°

    Args:
        path: CSV íŒŒì¼ ê²½ë¡œ

    Returns:
        ì½ì€ DataFrame
    """
    encodings = ["utf-8-sig", "utf-8", "cp949", "euc-kr"]
    for enc in encodings:
        try:
            return pd.read_csv(path, encoding=enc, low_memory=False)
        except Exception:
            continue
    # ìœ„ ì¸ì½”ë”©ìœ¼ë¡œë„ ì•ˆ ë˜ë©´ ìµœí›„ ì‹œë„
    return pd.read_csv(path, low_memory=False)


def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    DataFrameì˜ ì»¬ëŸ¼ëª…ì„ ì •ê·œí™” (ì†Œë¬¸ì, ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±°)

    Args:
        df: ì •ê·œí™”í•  DataFrame

    Returns:
        ì»¬ëŸ¼ëª…ì´ ì •ê·œí™”ëœ DataFrame
    """
    df = df.copy()
    df.columns = (
        pd.Series(df.columns)
        .astype(str).str.strip()
        .str.replace(r"\s+", "_", regex=True)
        .str.replace(r"[^\w]+", "_", regex=True)
        .str.lower()
    )
    return df


def rename_sensor_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    ì„¼ì„œ ë°ì´í„°ì˜ ì»¬ëŸ¼ëª…ì„ í‘œì¤€ ì´ë¦„ìœ¼ë¡œ ë³€ê²½

    Args:
        df: ë³€ê²½í•  DataFrame

    Returns:
        ì»¬ëŸ¼ëª…ì´ ë³€ê²½ëœ DataFrame
    """
    df = df.rename(columns={
        "ì˜¨ë„": "temp",
        "temp": "temp",
        "ìŠµë„": "humid",
        "ì••ë ¥": "press",
        "ë¶ˆëŸ‰ì—¬ë¶€": "error"
    })
    return df


def process_sensor_files(sensor_paths: List[Path], num_files: int = 2) -> Dict[str, Any]:
    """
    ì„¼ì„œ íŒŒì¼ë“¤ì„ ì½ê³  ì²˜ë¦¬í•˜ì—¬ ì •ë³´ ë°˜í™˜

    Args:
        sensor_paths: ì„¼ì„œ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        num_files: ì²˜ë¦¬í•  íŒŒì¼ ê°œìˆ˜ (ê¸°ë³¸ê°’: 2)

    Returns:
        ì²˜ë¦¬ëœ DataFrame ë¦¬ìŠ¤íŠ¸ì™€ íŒŒì¼ ì •ë³´
    """
    dfs_for_merge = []

    for idx, path in enumerate(sensor_paths[:num_files], start=1):
        df = read_csv_safely(str(path))
        df = normalize_columns(df)
        df = rename_sensor_columns(df)

        print(f"\n[ë‹¨ì¼ íŒŒì¼ í™•ì¸ {idx}] {path.name}")
        print("í–‰ ìˆ˜:", len(df))
        print("ì—´ ìˆ˜:", df.shape[1])
        print(df.tail(3))   # ë 3í–‰ ë¯¸ë¦¬ë³´ê¸°

        dfs_for_merge.append(df)

    return {
        "dataframes": dfs_for_merge,
        "file_count": len(dfs_for_merge)
    }


def get_sensor_files_summary(sensor_paths: List[Path]) -> Dict[str, Any]:
    """
    ëª¨ë“  ì„¼ì„œ CSV íŒŒì¼ì˜ ìš”ì•½ ì •ë³´ ë°˜í™˜

    Args:
        sensor_paths: ì„¼ì„œ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸

    Returns:
        íŒŒì¼ë³„ ì •ë³´ ë° ì´ í–‰ ìˆ˜
    """
    per_file = []
    for p in sensor_paths:
        dfi = read_csv_safely(str(p))
        per_file.append({"file": p.name, "rows": len(dfi), "cols": dfi.shape[1]})

    total_rows = sum(x["rows"] for x in per_file)

    print("\n[ì „ì²´ ì„¼ì„œ CSV ìš”ì•½]")
    print("íŒŒì¼ ìˆ˜:", len(per_file))
    print("ì´ í–‰ ìˆ˜ í•©ê³„:", total_rows)

    return {
        "file_count": len(per_file),
        "total_rows": total_rows,
        "per_file": per_file
    }


def merge_sensor_dataframes(dfs: List[pd.DataFrame]) -> pd.DataFrame:
    """
    ì—¬ëŸ¬ DataFrameì„ ë³‘í•©

    Args:
        dfs: ë³‘í•©í•  DataFrame ë¦¬ìŠ¤íŠ¸

    Returns:
        ë³‘í•©ëœ DataFrame
    """
    combined_df = pd.concat(dfs, ignore_index=True)

    print("\n[íŒŒì¼ ë³‘í•© ê²°ê³¼]")
    print("ìƒˆ ë°ì´í„°í”„ë ˆì„ í–‰ ìˆ˜:", len(combined_df))
    print("ìƒˆ ë°ì´í„°í”„ë ˆì„ ì—´ ìˆ˜:", combined_df.shape[1])
    print(combined_df.head(5))

    return combined_df


def save_combined_data(df: pd.DataFrame, output_dir: str = "artifacts", filename: str = "combined_data.csv") -> str:
    """
    ë³‘í•©ëœ ë°ì´í„°í”„ë ˆì„ì„ CSVë¡œ ì €ì¥

    Args:
        df: ì €ì¥í•  DataFrame
        output_dir: ì €ì¥í•  ë””ë ‰í† ë¦¬
        filename: íŒŒì¼ëª…

    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, filename)
    df.to_csv(output_path, index=False, encoding="utf-8-sig")

    print(f"\n[ì €ì¥ ì™„ë£Œ] {output_path}")
    return output_path


def preprocess_and_merge_sensors(
    data_dir: str = "data",
    num_files: int = 2,
    output_dir: str = "artifacts"
) -> Dict[str, Any]:
    """
    ì„¼ì„œ ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•© ì „ì²´ íŒŒì´í”„ë¼ì¸

    Args:
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
        num_files: ë³‘í•©í•  íŒŒì¼ ê°œìˆ˜
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬

    Returns:
        ì²˜ë¦¬ ê²°ê³¼ ì •ë³´
    """
    from pathlib import Path
    import re

    # ì„¼ì„œ íŒŒì¼ ìˆ˜ì§‘
    data_path = Path(data_dir)
    if not data_path.exists():
        raise FileNotFoundError(f"[ERROR] ë°ì´í„° í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {data_path}")

    sensor_paths = [
        p for p in data_path.glob("*.csv")
        if not re.search(r'error', p.name, flags=re.IGNORECASE)
    ]

    if len(sensor_paths) == 0:
        raise ValueError("ì„¼ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # íŒŒì¼ ì²˜ë¦¬
    processed = process_sensor_files(sensor_paths, num_files)

    # ì „ì²´ ìš”ì•½
    summary = get_sensor_files_summary(sensor_paths)

    # ë³‘í•©
    combined_df = merge_sensor_dataframes(processed["dataframes"])

    # ì €ì¥
    output_path = save_combined_data(combined_df, output_dir)

    return {
        "sensor_file_count": len(sensor_paths),
        "processed_file_count": processed["file_count"],
        "combined_rows": len(combined_df),
        "combined_cols": combined_df.shape[1],
        "output_path": output_path,
        "summary": summary
    }


# =========================================
#  ë°ì´í„° ì •ì œ í•¨ìˆ˜ë“¤
# =========================================

def check_missing_values(df: pd.DataFrame) -> Dict[str, Any]:
    """
    ê²°ì¸¡ì¹˜ í™•ì¸

    Args:
        df: í™•ì¸í•  DataFrame

    Returns:
        ê²°ì¸¡ì¹˜ ì •ë³´
    """
    missing_counts = df.isna().sum()
    total_missing = missing_counts.sum()

    print("ğŸ“‹ [ê²°ì¸¡ì¹˜ ê°œìˆ˜ í™•ì¸]")
    print(missing_counts)
    print(f"\nì´ ê²°ì¸¡ì¹˜ ê°œìˆ˜: {total_missing}")

    return {
        "missing_counts": missing_counts.to_dict(),
        "total_missing": int(total_missing)
    }


def remove_missing_values(df: pd.DataFrame, method: str = "drop") -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    ê²°ì¸¡ì¹˜ ì œê±° ë˜ëŠ” ëŒ€ì²´

    Args:
        df: ì²˜ë¦¬í•  DataFrame
        method: 'drop' (ì œê±°) ë˜ëŠ” 'fill' (í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´)

    Returns:
        ì²˜ë¦¬ëœ DataFrameê³¼ ì²˜ë¦¬ ì •ë³´
    """
    missing_before = df.isnull().sum().sum()

    if method == "drop":
        df_clean = df.dropna().reset_index(drop=True)
    elif method == "fill":
        df_clean = df.fillna(df.mean(numeric_only=True))
    else:
        raise ValueError("methodëŠ” 'drop' ë˜ëŠ” 'fill'ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    missing_after = df_clean.isnull().sum().sum()

    info = {
        "method": method,
        "missing_before": int(missing_before),
        "missing_after": int(missing_after),
        "rows_before": len(df),
        "rows_after": len(df_clean),
        "removed_rows": len(df) - len(df_clean)
    }

    print(f"\nê²°ì¸¡ì¹˜ {method} ì „: {missing_before}, í›„: {missing_after}")
    print(f"ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í›„ í–‰ ìˆ˜: {len(df_clean)}\n")

    return df_clean, info


def remove_outliers_zscore(df: pd.DataFrame, cols: List[str], threshold: float = 3.0) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    Z-score ê¸°ë°˜ ì´ìƒì¹˜ ì œê±°

    Args:
        df: ì²˜ë¦¬í•  DataFrame
        cols: ì´ìƒì¹˜ë¥¼ í™•ì¸í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        threshold: Z-score ì„ê³„ê°’ (ê¸°ë³¸ê°’: 3.0)

    Returns:
        ì •ì œëœ DataFrameê³¼ ì²˜ë¦¬ ì •ë³´
    """
    df = df.copy()
    z_scores = np.abs(stats.zscore(df[cols], nan_policy='omit'))
    mask = (z_scores < threshold).all(axis=1)
    clean_df = df[mask].reset_index(drop=True)

    removed_count = len(df) - len(clean_df)
    removal_rate = (removed_count / len(df)) * 100 if len(df) > 0 else 0

    info = {
        "threshold": threshold,
        "columns": cols,
        "rows_before": len(df),
        "rows_after": len(clean_df),
        "removed_rows": removed_count,
        "removal_rate": round(removal_rate, 2)
    }

    print("ğŸ“Š [Z-score ì´ìƒì¹˜ ì œê±° ê²°ê³¼]")
    print(f"ì œê±° ê¸°ì¤€: |z| > {threshold}")
    print(f"ì´ìƒì¹˜ ì œê±° ì „ í–‰ ìˆ˜: {len(df)}")
    print(f"ì´ìƒì¹˜ ì œê±° í›„ í–‰ ìˆ˜: {len(clean_df)}")
    print(f"ì´ ì œê±°ëœ í–‰ ìˆ˜: {removed_count}")
    print(f"ì œê±° ë¹„ìœ¨: {removal_rate:.2f}%\n")

    return clean_df, info


def clean_data(
    df: pd.DataFrame,
    num_cols: List[str],
    missing_method: str = "drop",
    outlier_threshold: float = 3.0,
    output_path: str = None
) -> Dict[str, Any]:
    """
    ë°ì´í„° ì •ì œ ì „ì²´ íŒŒì´í”„ë¼ì¸ (ê²°ì¸¡ì¹˜ ì œê±° + ì´ìƒì¹˜ ì œê±°)

    Args:
        df: ì •ì œí•  DataFrame
        num_cols: ì´ìƒì¹˜ ì œê±° ëŒ€ìƒ ìˆ˜ì¹˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        missing_method: ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²• ('drop' ë˜ëŠ” 'fill')
        outlier_threshold: Z-score ì„ê³„ê°’
        output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (ì„ íƒ)

    Returns:
        ì •ì œ ê²°ê³¼ ì •ë³´
    """
    print(f"ì›ë³¸ ë°ì´í„° í–‰ ìˆ˜: {len(df)}")
    print(f"ì›ë³¸ ë°ì´í„° ì—´ ìˆ˜: {df.shape[1]}\n")

    # ì‚¬ë³¸ ìƒì„±
    base_df = df.copy()

    # ê²°ì¸¡ì¹˜ í™•ì¸
    missing_info = check_missing_values(base_df)

    # ê²°ì¸¡ì¹˜ ì œê±°/ëŒ€ì²´
    df_no_missing, missing_result = remove_missing_values(base_df, method=missing_method)

    # ì´ìƒì¹˜ ì œê±°
    df_clean, outlier_result = remove_outliers_zscore(df_no_missing, num_cols, threshold=outlier_threshold)

    # ìµœì¢… ë°ì´í„° ì •ë³´ í™•ì¸
    print("âœ… [ìµœì¢… ì •ì œ ë°ì´í„° ì •ë³´]")
    print(df_clean.info())
    print("\nê²°ì¸¡ì¹˜ ê°œìˆ˜:")
    print(df_clean.isna().sum())
    print("\ní†µê³„ ìš”ì•½:")
    print(df_clean[num_cols].describe().round(3))

    # ì €ì¥ (ì„ íƒ)
    if output_path:
        df_clean.to_csv(output_path, index=False, encoding="utf-8-sig")
        print(f"\n[ì €ì¥ ì™„ë£Œ] {output_path}")

    return {
        "original_rows": len(df),
        "final_rows": len(df_clean),
        "total_removed_rows": len(df) - len(df_clean),
        "missing_info": missing_info,
        "missing_result": missing_result,
        "outlier_result": outlier_result,
        "cleaned_dataframe": df_clean,
        "output_path": output_path
    }
